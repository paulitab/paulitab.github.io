<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Assignments</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <header>
        <nav>
            <ul>
                <li><a href="index.html">About Me</a></li>
                <li><a href="work-experience.html">Work Experience</a></li>
                <li><a href="projects.html">Projects</a></li>
                <li><a href="contact.html">Contact</a></li>
                <li><a href="assignments.html">Assignments</a></li>
            </ul>
        </nav>
    </header>
    <main>
        <h1>My Assignments</h1>
        <ul>
            <li>Assignment 0: <a href="https://github.com/paulitab/paulalb-assignment-0" target="_blank">Github Link</a></li>
            <!-- explanation of how i approcahed it -->
             <p>For this assignment I created my personal website where you can find my contact, projects, work experience and assignment repos. I used html for the coding of the different sites and then uploaded them to github and launched them.</p>
            <p> The above link is to a dumy assignment which is a python scipt adding two numbers. </p>
        </ul>
        <ul>
            <li>Assignment 1: <a href="https://github.com/paulitab/paulalb-assignment-1/tree/main" target="_blank">Github Link</a></li>
            <!-- explanation of how i approcahed it -->
             <p>In this project, I analyzed elevator arrival data to determine the optimal position to wait in a lobby for the shortest walking distance to the next arriving elevator. By calculating average frequencies of elevator arrivals and testing different waiting locations, I developed a strategy that minimizes the average walking distance. Using both training and test datasets, I verified that the optimized location consistently outperformed the initial naive position, showing a measurable reduction in walking distance. </p>
        </ul>
        <ul>
            <li>Assignment 2: <a href="https://github.com/paulitab/paulalb-assignment-2/tree/main" target="_blank">Github Link</a></li>
            <!-- explanation of how i approcahed it -->
            <p>For this assignment, I created a KMeans clustering visualization tool using Plotly and JavaScript. The tool allows users to generate datasets, run KMeans clustering to convergence, step through KMeans iterations, and reset the plot. The visualization displays the dataset and the cluster centroids, updating with each iteration of KMeans. The user can select the number of clusters (k) and the initialization method (random, KMeans++, farthest first, or manual) to customize the clustering process. The tool provides an interactive and informative way to explore KMeans clustering and understand how the algorithm works.</p>
            <p>Here is the Demo Video</p>
            <p><iframe width="560" height="315" src="https://www.youtube.com/embed/CJGe07rT8AE" frameborder="0" allowfullscreen></iframe></p>
            <p><a href="https://youtu.be/CJGe07rT8AE" target="_blank">K Means Clustering Demo Video Paula Lopez Burgos</a></p>
        </ul>
        <ul>
            <li>Assignment 3: <a href="https://github.com/paulitab/paulalb-assignment-3" target="_blank">Github Link</a></li>
            <p>In this project, I implemented Singular Value Decomposition (SVD) to compress an image by decomposing each RGB channel and retaining only the most important singular values based on a specified rank. I then reconstructed the image with reduced dimensionality, preserving key visual details while significantly reducing the data size. </p>
        </ul>
        <ul>
            <li>Assignment 4: <a href="https://github.com/paulitab/paulalb-assignment-4" target="_blank">Github Link</a></li>
            <p>For this assignment, I loaded and preprocessed the BBC News dataset, created a term-document matrix using TF-IDF, applied SVD to reduce dimensionality, and identified key terms to label each topic.</p>
            <p>Here is the Demo Video</p>
            <p><iframe width="560" height="315" src="https://www.youtube.com/embed/8vd2Tqkv3tI" frameborder="0" allowfullscreen></iframe></p>
            <p><a href="https://youtu.be/8vd2Tqkv3tI" target="_blank">LSA Search Engine Demo Video Paula Lopez Burgos</a></p>
        </ul>
        <ul>
            <li>Assignment 5: <a href="https://github.com/paulitab/paulalb-assignment-5" target="_blank">Github Link</a></li>
            <p>In this assignment, I developed a K-Nearest Neighbors (KNN) model to predict customer churn. I focused on preprocessing the data, implementing KNN from scratch, tuning hyperparameters using cross-validation, and evaluating the model with ROC-AUC scores. Finally, I trained the model on the full dataset and made predictions on the test data. </p>
            <p>The best k was 37 with the best distance metric being manhattan distance</p>
        </ul>
        <ul>
            <li>MIDTERM: <a href="https://github.com/paulitab/cs506_midterm" target="_blank">Github Link</a></li>
            <p>In this Midterm we had to develop a model that predicted the scores of Amazon Movie Reviews from the dataset given.</p>
            <p>The github link above contains the following:</p>
            <p>
                <li>The final predictive model code</li>
                <li>The code for gradient boosting with TF-IDF (lower accuracy) <a href="https://github.com/paulitab/cs506_midterm/" target="_blank" </a></li>
                <li>The code for KNN classifier <a href="https://github.com/paulitab/cs506_midterm/" target="_blank" </a></li>
                <li>The code for Ridge Classifier <a href="https://github.com/paulitab/cs506_midterm/" target="_blank" </a></li>
                <li>The report with the results and analysis</li>
            </p>
        </ul>
    </main>
</body>
</html>
